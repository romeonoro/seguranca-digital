# Protocolos de Exclus√£o de Rob√¥s

## üìå O que √©
O **protocolo de exclus√£o de rob√¥s** (Robots Exclusion Protocol ‚Äì **robots.txt**) √© um **padr√£o de comunica√ß√£o entre sites e crawlers/bots** (como Googlebot, Bingbot, etc.), criado para **indicar quais partes de um site podem ou n√£o ser acessadas e indexadas** por mecanismos de busca e outros rob√¥s automatizados.

Ele **n√£o √© uma regra obrigat√≥ria**, mas sim uma **orienta√ß√£o**: os bots leg√≠timos (como buscadores) respeitam o protocolo, mas bots maliciosos podem ignor√°-lo.

---

## üìå Para que serve
- **Controle de indexa√ß√£o**: evitar que p√°ginas espec√≠ficas apare√ßam em resultados de busca (ex: p√°ginas de teste, duplicadas ou privadas).  
- **Economia de recursos**: impedir que rob√¥s acessem √°reas pesadas ou irrelevantes, poupando banda e processamento do servidor.  
- **Organiza√ß√£o de SEO**: direcionar os buscadores para o conte√∫do mais relevante.  
- **Seguran√ßa b√°sica**: evitar exposi√ß√£o desnecess√°ria de diret√≥rios (embora n√£o substitua medidas reais de seguran√ßa).

---

## üìå Como funciona
O controle √© feito por meio de um arquivo chamado **`robots.txt`**, colocado **na raiz do site** (exemplo: `https://www.exemplo.com/robots.txt`).  

Esse arquivo cont√©m regras em formato simples:

```txt
User-agent: *
Disallow: /admin/
Disallow: /privado/

User-agent: Googlebot
Allow: /blog/
Disallow: /teste/
```

- `User-agent:` define a qual bot a regra se aplica (`*` = todos).  
- `Disallow:` bloqueia diret√≥rios ou p√°ginas.  
- `Allow:` permite acesso (√∫til quando um diret√≥rio √© bloqueado, mas h√° exce√ß√µes).  

---

## üìå Onde se utiliza
- **Sites e portais**: para organizar SEO e evitar indexa√ß√£o de partes internas.  
- **E-commerces**: bloqueando p√°ginas de carrinho, filtros ou sess√µes de usu√°rio.  
- **Sistemas web**: para impedir que √°reas administrativas ou de teste sejam varridas por bots.  
- **APIs e servi√ßos online**: quando n√£o se deseja indexa√ß√£o autom√°tica.  

Al√©m disso, buscadores tamb√©m consultam o arquivo **`sitemap.xml`** (muitas vezes indicado dentro do `robots.txt`) para descobrir p√°ginas que devem ser indexadas.

---

üëâ **Resumindo**: o **protocolo de exclus√£o de rob√¥s** √© uma forma de **dialogar com crawlers**, dizendo o que pode ou n√£o ser explorado, melhorando o desempenho do site e a relev√¢ncia nos buscadores.
